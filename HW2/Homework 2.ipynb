{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will be building several varieties of language models.\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you construct the following models in PyTorch:\n",
    "\n",
    "1. A trigram model with linear-interpolation. $$p(y_t | y_{1:t-1}) =  \\alpha_1 p(y_t | y_{t-2}, y_{t-1}) + \\alpha_2 p(y_t | y_{t-1}) + (1 - \\alpha_1 - \\alpha_2) p(y_t) $$\n",
    "2. A neural network language model (consult *A Neural Probabilistic Language Model* http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "3. An LSTM language model (consult *Recurrent Neural Network Regularization*, https://arxiv.org/pdf/1409.2329.pdf) \n",
    "4. Your own extensions to these models...\n",
    "\n",
    "\n",
    "Consult the papers provided for hyperparameters.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will use of this problem is known as the Penn Treebank (http://aclweb.org/anthology/J93-2004). It is the most famous dataset in NLP and includes a large set of different types of annotations. We will be using it here in a simple case as just a language modeling dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we input our data. Here we will use the first 10k sentences of the standard PTB language modeling split, and tell it the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format for language modeling is strange. We pretend the entire corpus is one long sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 1\n"
     ]
    }
   ],
   "source": [
    "print('len(train)', len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the vocab itself. (This dataset has unk symbols already, but torchtext adds its own.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 10001\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging you may want to use a smaller vocab size. This will run much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    TEXT.build_vocab(train, max_size=1000)\n",
    "    len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batching is done in a strange way for language modeling. Each element of the batch consists of `bptt_len` words in order. This makes it easy to run recurrent models like RNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what these batches look like. Each is a string of length 32. Sentences are ended with a special `<eos>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text batch [max bptt length, batch size] torch.Size([32, 10])\n",
      "Second in batch Variable containing:\n",
      "    8\n",
      "  202\n",
      "   77\n",
      "    5\n",
      "  183\n",
      "  561\n",
      " 3837\n",
      "   18\n",
      "  975\n",
      "  976\n",
      "    7\n",
      "  943\n",
      "    5\n",
      "  157\n",
      "   78\n",
      " 1571\n",
      "  289\n",
      "  645\n",
      "    3\n",
      "   30\n",
      "  132\n",
      "    0\n",
      "   20\n",
      "    2\n",
      "  273\n",
      " 7821\n",
      "   17\n",
      "    9\n",
      "  117\n",
      " 2815\n",
      "  969\n",
      "    6\n",
      "[torch.LongTensor of size 32]\n",
      "\n",
      "Converted back to string:  in part because of buy programs generated by stock-index arbitrage a form of program trading involving futures contracts <eos> but interest <unk> as the day wore on and investors looked ahead to\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it) \n",
    "print(\"Size of text batch [max bptt length, batch size]\", batch.text.size())\n",
    "print(\"Second in batch\", batch.text[:, 2])\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 2].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next batch will be the continuation of the previous. This is helpful for running recurrent neural networks where you remember the current state when transitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted back to string:  the release later this week of two important economic reports <eos> the first is wednesday 's survey of purchasing managers considered a good indicator of how the nation 's manufacturing sector fared\n"
     ]
    }
   ],
   "source": [
    "batch = next(it)\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 2].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no separate labels. But you can just use an offset `batch.text[1:]` to get the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "Using the data given by this iterator, you should construct 3 different torch models that take in batch.text and produce a distribution over the next word. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw2-s18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final Kaggle test, we will have you do a next word prediction task. We will provide a 10 word prefix of sentences, and it is your job to predict 10 possible next word candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but while the new york stock exchange did n't fall ___\r\n",
      "some circuit breakers installed after the october N crash failed ___\r\n",
      "the N stock specialist firms on the big board floor ___\r\n",
      "big investment banks refused to step up to the plate ___\r\n",
      "heavy selling of standard & poor 's 500-stock index futures ___\r\n",
      "seven big board stocks ual amr bankamerica walt disney capital ___\r\n",
      "once again the specialists were not able to handle the ___\r\n",
      "<unk> james <unk> chairman of specialists henderson brothers inc. it ___\r\n",
      "when the dollar is in a <unk> even central banks ___\r\n",
      "speculators are calling for a degree of liquidity that is ___\r\n"
     ]
    }
   ],
   "source": [
    "!head input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sample Kaggle submission, let us build a simple unigram model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count = Counter()\n",
    "for b in iter(train_iter):\n",
    "    count.update(b.text.view(-1).data.tolist())\n",
    "count[TEXT.vocab.stoi[\"<eos>\"]] = 0\n",
    "predictions = [TEXT.vocab.itos[i] for i, c in count.most_common(20)]\n",
    "with open(\"sample.txt\", \"w\") as fout: \n",
    "    print(\"id,word\", file=fout)\n",
    "    for i, l in enumerate(open(\"input.txt\"), 1):\n",
    "        print(\"%d,%s\"%(i, \" \".join(predictions)), file=fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,word\r\n",
      "1,the <unk> N of to a in and 's for that is $ said it on by at as from\r\n",
      "2,the <unk> N of to a in and 's for that is $ said it on by at as from\r\n",
      "3,the <unk> N of to a in and 's for that is $ said it on by at as from\r\n",
      "4,the <unk> N of to a in and 's for that is $ said it on by at as from\r\n",
      "5,the <unk> N of to a in and 's for that is $ said it on by at as from\r\n",
      "6,the <unk> N of to a in and 's for that is $ said it on by at as from\r\n",
      "7,the <unk> N of to a in and 's for that is $ said it on by at as from\r\n",
      "8,the <unk> N of to a in and 's for that is $ said it on by at as from\r\n",
      "9,the <unk> N of to a in and 's for that is $ said it on by at as from\r\n"
     ]
    }
   ],
   "source": [
    "!head sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric we are using is mean average precision of your 20-best list. \n",
    "\n",
    "$$MAP@20 = \\frac{1}{|D|} \\sum_{u=1}^{|D|} \\sum_{k=1}^{20} Precision(u, 1:k)$$\n",
    "\n",
    "Ideally we would use log-likelihood or ppl as discussed in class, but this is the best Kaggle gives us. This takes into account whether you got the right answer and how highly you ranked it. \n",
    "\n",
    "In particular, we ask that you do not game this metric. Please submit *exactly 20* unique predictions for each example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always you should put up a 5-6 page write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
